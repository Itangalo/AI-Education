# Part 3: More about AI

# 12: The Background of ChatGPT
ChatGPT was launched as a beta on November 30, 2022, by the company OpenAI and quickly gained attention. In just five days, the number of users had grown to one million[^1], something that normally takes months or years even for successful technology services[^2].

Earlier in 2022, several AI services for images had also gained attention. The most well-known of these tools are DALL-E 2 (also from OpenAI), Midjourney, and Stable Diffusion. Although earlier AI services could create realistic images (see, for example, [thispersondoesnotexist.com][3] from 2019), it was not until 2022 that you could write a description of almost anything and get back an image that looked good. Although not all images are world-class, there are also examples of AI-created images that [win competitions against images created by humans][4].

Unlike AI for generating images (and [a long list of other AI services][5]), ChatGPT can be considered a more generally competent AI, not trained for a specific task and capable of handling such diverse things as giving dietary advice, writing code, and imitating a Linux terminal.

The AI model behind ChatGPT is called GPT-3.5 and is a variant of GPT-3, which was launched in 2020. The success of ChatGPT was largely due to the fact that it became easy for anyone to use the AI. GPT stands for "generative pre-trained transformer" and is an abbreviation used by OpenAI for a series of its AI models. They are a type of model called a large language model (LLM), which in turn uses a more general principle called artificial neural networks – a technique inspired by how nerve cells in our brains work.

The capacity of artificial neural networks is often measured in the number of so-called parameters they have, roughly equivalent to the connections between nerve cells in our brains. These parameters are values set during AI training to provide the best answers possible. Training large AI models requires huge amounts of data and lots of computer power, which has the effect that basically only states and large technology companies that can create large AI models.[^3] Often parts of the data also need to be assessed or labeled by humans, making it even more expensive to produce. A major difference between GPT-3 and GPT-3.5 is that many texts have been assessed by humans, so that ChatGPT can understand what is classified as great or inappropriate replies[^4]. The finished models are valuable assets for the company. However, using the finished models is much less resource-intensive and can be done on single personal computers.

OpenAI is tight-lipped about information regarding ChatGPT. In interviews, they have stated that the model was trained on a significant portion of the text available on the internet in 2021 (and it's clear that it doesn't retrieve new information from the web). It's difficult to find information about how many parameters GPT-3.5 has, but GPT-3 has 175 billion parameters.[^5] Two different sources suggest that GPT-3.5 (and therefore ChatGPT) only has a tenth of that number of parameters, around 2 billion, which would match some information found on OpenAI’s blog.[^6]

OpenAI is likely to release its follow-up, GPT-4, in 2023. Rumors about how many parameters it will have range from as many as GPT-3 to more than five hundred times as many[^7]. According to the German AI Association, Beijing Academy of AI and Google already had undisclosed language models in 2021 with around one hundred times as many parameters as GPT-3.[^8]

OpenAI started as a non-profit company, with a strong focus on making AI accessible to many. When Microsoft entered the company in 2019, OpenAI became for-profit. The trial version of ChatGPT is free, and each ChatGPT conversation is said to cost ”single-digit cents" in calculations[^9]. On February 1st [ChatGPT Plus][8] was introduced for the US market – a paid subscription that gives faster and more reliable response times, as well as priority access to new features and improvements.

# 13: How does this affect our view of knowledge?
There are clear gaps in ChatGPT's knowledge, and it can be amusingly confident even when it's wrong. At the same time, it's also clear that many are amazed by what the AI can do, and that computers have taken yet another step into the area that we thought was reserved for humans.

What does technology like this mean for our view of knowledge?

It's too early to give an answer to that question, not least because the technology is still accelerating. It's nonetheless a question worth exploring. It's reasonable to believe that parts of what we today consider important knowledge will be reshaped, and some things we will no longer consider important. It's also likely that many things will remain unchanged.

An interesting paradox is that despite ChatGPT's ability to answer anything, it is so unreliable that it is difficult to use as a source of information for things you do not already know well or are willing to spend considerable time double-checking. Put in another way: To use ChatGPT in a meaningful way requires judgement, and for that judgement requires knowledge of what you use ChatGPT for[^10]. An experienced teacher can use ChatGPT as a support for making lesson plans, creating materials and getting ideas on how the teaching can be improved, but an inexperienced teacher relying on ChatGPT risks falling into AI pitfalls or wasting time.

There is a parallel here, to access to information on the internet, that is a bit striking: It is sometimes said that the internet has made it unnecessary to know facts, because one can always look up what one needs to know. This is true when it comes to trivial facts ("What is the capital of Azerbaijan?"), but to take advantage of deeper information, one needs to understand the context, be able to connect different concepts, and even have a sense of whether the information one receives is revolutionizing, doubtful, or fully normal. To be able to make use of facts on the internet, you must must know a lot of things yourself. And to be able to use ChatGPT as an assistant in an area, you must have competence in the area yourself.

A first guess is therefore that ChatGPT will be able to speed up processes of analyzing and summarizing texts, and also writing well-formulated texts from outlines or drafts. These are things we can do today, but the technology makes it faster for us.

To explore the consequences further it could illuminating to compare with when calculators became common. Broadly speaking, the impact of the calculator can be summarized as follows:

1. Many warned that we would become worse at doing arithmetics in our heads and by hand.
2. We probably did become worse at doing arithmetics in our heads and by hand, on average. Fewer people received the training needed to become good at doing arithmetics when calculators were available.
3. Basic arithmetic skills, without a calculator, are still considered important both inside and outside of school.
4. The ability to use a calculator is now considered a part of being able to do mathematics.
5. Things like multiplying large numbers and calculating square roots is no longer done by hand, except as a brain exercise or a party trick. Being able to perform extensive calculations by hand just isn’t a meaningful skill any more.
6. The proportion of people who can multiply large numbers and calculate square roots when needed has increased, provided that calculators are available (although fewer understand what a square root is and when to calculate it).
7. The ability to perform extensive calculations (with tools more powerful than pocket calculators) has opened up new fields in mathematics.

What could this list mean, translated to using ChatGPT to summarize and write texts?
1. We are concerned that our ability to read and summarize/write texts “by hand” will deteriorate.
2. Our ability to do this will probably deteriorate.
3. Being able to summarize and write texts on a basic level will continue to be an important skill, both inside and outside of school.
4. Being able to use AI to summarize and write texts will become an important skill to learn.
5. Reading through extensive texts and making literature reviews will not be something we do by hand, unless we do it for fun. It will also not be as important to be able to structure texts and use simple language.
6. The fact that texts are long will prevent fewer people than before from accessing their content, and more will be able to produce well-written and well-structured texts. At the same time, fewer may be able to explain why a text is easy to read or are capable of structuring texts without AI help.
7. More powerful technology will provide opportunities to process, summarize, and even synthesize large amounts of text, which will give us new insights, new ways of thinking, and new perspectives.

The points above are, of course, only speculation and more or less directly translated from the case of calculators (which in turn should be acknowledged is more of a shot in the dark than based in research). But they are a way to explore what the future _may_ look like.

But it's important to remember that ChatGPT is not just capable of summarizing and writing texts. The AI is extremely versatile and can write computer programs, analyze the tone of text, generate text-based games, give comfort, provide dietary advice, translate texts and come up with new episodes of a TV series. Not with perfect quality, but you could still make many different lists similar to the one above to explore what technology like ChatGPT can lead to.

A strong limitation of ChatGPT is how unreliable it is when it comes to facts and logic. You might think that it's "just" about adding functions that let an AI check if what it says is logically consistent or compare its answers to reliable sources. But the model underlying ChatGPT is based _only_ on a universe built of words – there are no concepts of what is true or not, just what is more or less natural/likely sequences of words. Adding logic or a notion of what is true would likely require [another type of technology][9], and is therefore not "just". Nonetheless, we are likely to take steps in that direction in 2023. The AI technology we see today is light-years better than [what was available in 2019][10] – and already back then, it elicited attention, questions, and concern. GPT-3.5, the underlying model for ChatGPT, is expected to be replaced by a much more capable GPT-4 in 2023. [Several other companies and organizations have similar services that have not been launched yet][11], some of which are expected to be much more powerful and there are even examples that can mix media such as images and text.

It's a big leap, but if we imagine that AI in the near future can do most things that do not require too much mental effort, we can get this slightly worrying list:

1. We are concerned that our ability to do mental work will deteriorate, such as analyzing new topics, do logical reasoning and drawing conclusions.
2. We are also likely to become worse at this.
3. Being able to perform basic mental work, such as analyzing new topics, do logical reasoning and drawing conclusions, will continue to be important skills.
4. Being able to use AI to analyze new topics, do logical reasoning and draw conclusions will be an important skill to learn.
5. We will not perform advanced analyses or track complex logical reasoning without AI assistance, unless we do it for fun.
6. Advanced and large topics, or complex reasoning, will become accessible to more. At the same time, fewer may understand what it means that conclusions are well supported.
7. With more powerful technology, it will be possible to think and reason further and deeper than humans can do ”by hand”. This will give us new types of insights and knowledge areas to explore.

Expressed in prose: We will generally become worse at "by hand" understanding new things, do logical reasoning and draw conclusions when AI can do it for us, despite these things still being emphasized in education. On the other hand, most people will learn to use AI when complex reasoning is required or when learning new things. This will give more people the opportunity to feel that they understand many different topics – even if many cannot explain the reasoning and conclusions that their sense of understanding is based on, and also have a limited understanding of what it means that conclusions are well supported. When applied in large scale, AI technology can be used to reason and draw conclusions about many more things than a human can do in their lifetime, giving us new types of insights and knowledge areas to explore.

# 14: AI and Security
A book about AI should also address the risks of AI. Concerns that AI could seriously harm society, the world order, or humanity are well-founded enough that many serious thinkers, researchers, and lobbyists are working to understand and reduce those risks.

The risks of AI increase as AI becomes more competent. There are several types of risks, but three broad categories are:
1. The risk that AI is used to benefit a few while hurting many.
2. The risk that the goals we give AI lead to unintended and harmful consequences.
3. The risk that AI creates its own goals that do not align with ours.

These risks partially overlap with each other, and there are risks that fall outside of these categories.

## Risk of AI benefiting few and hurting many
In this category, there are several existing AI technologies. One example is surveillance cameras with facial recognition technology used in China, which increases the ability to prevent and follow up on crimes - which for example also includes demonstrating for democracy in Hong Kong. Another example is drones used in warfare (for example in Libya), which can identify targets and decide whether to attack them.

A much more diffuse example is the AI systems that determine what content users on social media should see. They are powerful enough to create profits for the companies that own the platforms, but at the same time cause harm in the form of, for example, mental illness, polarization in society, and spread of fake news.

There is a particular risk with highly competent AI, in that it is such a cheap and accessible technology to use. While things like nuclear weapons are difficult to create or purchase for terrorist organizations, the price of armed AI drones is so low that they could be purchased in thousands. Someone who wants to do a lot of harm could also use AI normally used for finding potential medicines to create chemical weapons. [^11] While creating AI models is expensive, using them is cheap, and democratization of technology that can be used for mass destruction carries great risks.

## Risk of Unexpected and Harmful Consequences
Even if high-competent AI is in the hands of people who mean well, there is a risk of bad consequences. To a large extent, this is due to the fact that AI can lead to a few stated goals being pursued so effectively that other things suffer - things that we did not think of and perhaps did not have the chance to anticipate when we formulated the goals for the AI.

Algorithms used in social platforms are an example of this: The goal of the algorithms is to keep people as engaged as possible - they should continue to read and continue to click (which in itself has to do with a large user base and advertising revenues, or for that matter collecting data that can be used to train AI). What's wrong with an AI helping to show you things that you find interesting?

One of the problems arises when it turns out that people are engaged by content that makes them upset, which in turn leads to social platforms often showing posts and news that create polarization both online and in society. Further problems arise when the algorithms do not take into account what is true, but only what creates more clicks.

In a hypothetical future with a super-AI, it becomes extremely important what goals we have given the AI, and it turns out that it is not easy to find any goals that do not carry a risk of going astray. The goal "people should be happy" sounds innocent, but for example, it can be achieved through a morphine drip. Even very limited goals have a great potential to go astray. It is sometimes expressed with the phrase "you can't fetch coffee if you're dead", meaning that a super-AI with the sole goal of fixing coffee will realize that it cannot make coffee if it is shut off - which in turn can lead to all sorts of measures to get rid of things that can shut it off (such as people).

## Super-AI Creating its Own Goals
AI researchers are divided on the likelihood of inventing a super-AI - an AI that is at least as good as a human at solving virtually all types of problems. Some researchers consider it impossible or extremely unlikely in the near future, but the average assessment has clearly moved closer in recent years. The values ​​vary between surveys, and the median estimate is 100, 50 or 30 years.

Super-AI is often referred to as "strong AI" or "artificial general intelligence" (AGI), and many of those working in the field see it as important that we take the time to understand and reduce the risks of a super-AI: even if the likelihood of creating a super-AI within a hundred years is as low as 10 percent, it would be well invested efforts.

One thing that makes super-AI particularly risky is the possibility of an AI explosion: if a super-AI is at least as good as humans at solving problems, it also includes the ability to create new, better AI. The new AI becomes even better at this, which in a relatively short time could lead to an AI that outclasses the thought capability that humans can assemble.

Some of the research in AI security therefore focuses on what is called AI alignment - ensuring that the goals of the AI align with what humanity considers good. Part of this involves finding ways to secure that the AI understands the goals we give it, that the AI follows these goals, and that the AI cannot change these goals itself. A significant problem is that AI in the form of neural networks is largely a black box, where we can see what comes out but cannot see how the AI has come up with a particular answer or decision.

Research in AI security is moving forward, but one problem is that economic, military and other interests are making AI technology progress much faster.

"Can't we just pull the plug on a super-AI if it shows a desire to harm us humans?" Perhaps. If it really is a super-AI, it understands humans well enough to know where the boundary goes, and is capable of securing its survival even if it begins to occur at the expense of humans. We can compare with the fossil industry and climate change: it is beyond all reasonable doubt that our emissions of greenhouse gases, largely due to the fossil industry, harm humanity as a whole - in a serious and expensive way. Yet, we are unable to act to stop it.

## Other Risks
A few other risks associated with increasingly sophisticated AI are briefly described below.

* **Skewed power structures can be reinforced.** Because AI technology is largely based on training with existing data, there is a clear risk that skewed power structures will continue to exist in AI decisions. For example, if an AI is to recommend suitable candidates for a management position, there is a risk that white middle-aged men will receive unjustified advantages.
* **Human content can be drowned out by AI-generated content.** Given that GPT-3.5 has been trained on text equivalent to 57 billion human-life worth of reading, it can be stated that most of the text on the internet is _not_ created by humans. As AI becomes increasingly sophisticated, it will become more difficult to find text that has actually been written by a human, and more difficult to know when one has found it.
* **Resources and power can become even more unequal.** Although AI is relatively cheap to use, it is expensive to produce and requires vast amounts of data. The few actors that can produce powerful AI can wield significant influence - both in terms of economic production and in terms of the information that people are exposed to. The large generative AI models available today come from only six actors. [^12]
* **Rapid changes in the job market.** AI development can lead to a contraction in the job market or to changes in the skills that are in demand, resulting in many people becoming unemployed. This can cause problems for the affected individuals and unrest at a societal level.

[^1]:	[https://twitter.com/gdb/status/1599683104142430208][1]

[^2]:	[https://indianexpress.com/article/technology/chatgpt-hit-1-million-users-5-days-vs-netflix-facebook-instagram-spotify-mark-8394119/][2]

[^3]:	https://arxiv.org/abs/2212.08073

[^4]:	OpenAI has faced criticism for how those who made these assessments were treated, see https://time.com/6247678/openai-chatgpt-kenya-workers/

[^5]:	See for example https://www.datacamp.com/blog/what-we-know-gpt4

[^6]:	[https://openai.com/blog/instruction-following/][6]

[^7]:	https://www.datacamp.com/blog/what-we-know-gpt4

[^8]:	Reported in article at https://the-decoder.com/ai-in-education-chatgpt-is-just-the-beginning/

[^9]:	[https://fortune.com/longform/chatgpt-openai-sam-altman-microsoft/][7]

[^10]:	The term "judgment" here is quite loose. One could, hypothetically, imagine someone using ChatGPT to create a bomb, which is quite thoughtless. Someone who follows ChatGPT's instructions without having knowledge of explosive materials risks harming themselves or wasting resources, while someone with "judgment" in bomb making can see where the instructions are reasonable, where they are wrong or gaps, and can relate to new things that AI suggests.

[^11]:	https://futureoflife.org/podcast/sean-ekins-on-regulating-ai-drug-discovery/

[^12]:	https://arxiv.org/abs/2301.04655

[1]:	https://twitter.com/gdb/status/1599683104142430208
[2]:	https://indianexpress.com/article/technology/chatgpt-hit-1-million-users-5-days-vs-netflix-facebook-instagram-spotify-mark-8394119/
[3]:	https://thispersondoesnotexist.com/
[4]:	https://www.nytimes.com/2022/09/02/technology/ai-artificial-intelligence-artists.html
[5]:	https://theresanaiforthat.com/
[6]:	https://openai.com/blog/instruction-following/
[7]:	https://fortune.com/longform/chatgpt-openai-sam-altman-microsoft/
[8]:	https://openai.com/blog/chatgpt-plus/
[9]:	https://writings.stephenwolfram.com/2023/01/wolframalpha-as-the-way-to-bring-computational-knowledge-superpowers-to-chatgpt/
[10]:	https://www.theguardian.com/technology/2019/feb/14/elon-musk-backed-ai-writes-convincing-news-fiction
[11]:	https://anita.beehiiv.com/p/chatgpt-alternatives