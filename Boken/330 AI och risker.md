# AI och risker
En bok som handlar om AI bör också ta upp risker med AI. Oro för att AI kan komma att allvarligt skada samhället, världsordningen eller mänskligheten är tillräckligt välgrundade för att många seriösa tänkare, forskare och hela organisationer arbetar för att förstå och minska de riskerna.

Riskerna med AI ökar ju mer kompetent AI:n är. Det finns flera typer av risker, men tre breda kategorier är:
1. Risk att AI används för att gynna få samtidigt som det skadar många.
2. Risk att de mål vi ger AI leder till oförutsedda och skadliga konsekvenser.
3. Risk att AI skapar egna mål som inte stämmer med våra.

De här riskerna går delvis in i varandra, och det finns risker som faller utanför de här kategorierna.

## Risk att AI gynnar få och skadar många
I den här kategorin finns ett antal befintliga AI-tekniker. Ett exempel är övervakningskameror med ansiktsigenkänningsteknik som används i Kina, vilket ökar möjligheten att förebygga och följa upp brott – vilket till exempel även omfattar att demonstrera för demokrati i Hongkong. Ett annat exempel är drönare som används i krigsföring (exempelvis i [Libyen][1]), som själva kan identifiera mål och avgöra om de ska attackeras.

Ett mycket mer diffust exempel är de AI-system som bestämmer vilket innehåll användare på sociala medier ska se. De är tillräckligt kraftfulla för att skapa vinster för de företag som äger plattformarna, men orsaker samtidigt skada i form av exempelvis psykisk ohälsa, polarisering i samhället och spridning av osanningar.

Det finns en särskild risk med högkompetent AI, i och med att det är så tillgängligt och billigt att använda. Medan saker som kärnvapen är svåra att skapa eller köpa för terroristorganisationer, så är priset för beväpnade AI-drönare så pass lågt att de skulle kunna köpas i tusentals. Någon som vill göra mycket skada skulle också kunna ta AI som normalt används för att hitta potentiella läkemedel för att skapa kemiska stridsmedel.[^1] Medan det är dyrt att skapa AI-modeller är det billigt att använda dem, och demokratisering av teknik som kan användas för massförstörelse för med sig stora risker.

## Risk för oförutsedda och skadliga konsekvenser
Även om högkompetent AI sitter i händer på folk som vill väl finns det risk för dåliga konsekvenser. I stor utsträckning hänger det samman med att AI kan leda till att ett fåtal uttalade mål eftersträvas så effektivt att andra saker blir lidande – saker som vi inte tänkte på och kanske inte hade chans att förutse när vi formulerade målen för AI:n.

Algoritmer som används i sociala plattformar är ett exempel på detta: Målet med algoritmerna är att i så hög utsträckning som möjligt hålla människor engagerade – de ska fortsätta läsa och fortsätta klicka (vilket i sig har att göra med en bred användarbas och annonsintäkter, eller för den delen att samla in data som kan användas för att träna AI). Vad är fel med att en AI hjälper till med att visa saker som du tycker är intressanta?

Ett av problemen uppstår när det visar sig att människor blir engagerade av innehåll som gör dem upprörda, vilket i sin tur leder till att sociala plattformar ofta visar inlägg och nyheter som skapar polarisering både på nätet och i samhället. Ytterligare problem uppstår när algoritmerna inte tar hänsyn till vad som är sant, utan bara vad som skapar mer klick. De problemen blir ännu större när algoritmer börjar påverka vilka åsikter människor har, och skapar starkt polariserade grupper av människor som är lätta att göra upprörda – och få klick från.

I en hypotetisk framtid med en super-AI blir det extremt viktigt vilka mål vi gett AI:n, och det visar sig att det inte är enkelt att hitta några mål som inte riskerar att spåra ur. Målet ”människor ska vara lyckliga” låter oskyldigt, men kan till exempel uppnås genom morfin-dropp. Även mycket begränsade mål har stor potential att spåra ur. Det uttrycks ibland med frasen ”you can’t fetch coffee if you’re dead”, vilket står för att en super-AI med det enda målet att fixa kaffe kommer att inse att den inte kan göra kaffe om den stängs av – vilket i sin tur kan leda till alla möjliga åtgärder för att få bort sådant som kan stänga av den (så som människor).

## Super-AI som skapar egna mål
Forskare inom AI är oense om hur troligt det är att vi kommer att uppfinna en super-AI – en AI som är minst lika bra som en människa på i princip all typ av problemlösning. Vissa forskare bedömer det som omöjligt eller extremt osannolikt inom överskådlig tid, men den genomsnittliga bedömningen har krupit tydligt närmare nutik de senaste åren. De värden som anges varierar mellan olika undersökningar, och mediangissningen anges till 100, 50 eller 30 år.

Super-AI omnämns ofta som ”strong AI” eller ”artificial general intelligence” (AGI), och många av de som arbetar inom området ser det som angeläget att vi lägger möda på att förstå och minska riskerna med en super-AI: Även om sannolikheten för att vi skapar en super-AI inom hundra år är så liten som 10 procent vore det väl investerade insatser.

En viktig sak som gör super-AI särskilt riskfyllt är möjligheten till en AI-explosion: Om en super-AI är minst lika bra som människor på problemlösning, så omfattar det även förmågan att skapa ny, bättre AI. Den nya AI:n blir ännu bättre på detta, vilket på förhållandevis kort tid skulle kunna leda till en AI som utklassar den tankeförmåga som människor kan samla ihop.

En del av forskningen inom AI-säkerhet fokuserar därför på det som kallas _AI alignment_ – att de mål som AI har ska sammanfalla med sådant som mänskligheten tycker är bra. Delar av detta handlar om att hitta sätt att säkra att AI förstår de mål som vi ger, att AI följer de målen, och att AI:n själv inte kan ändra målen. Ett betydande problem är att AI i form av neurala nätverk i mycket stor utsträckning är svarta lådor, där man ser vad som kommer ut men inte kan se hur AI:n kommit fram till ett visst svar eller ett visst beslut.

Forskning på AI-säkerhet går framåt, men ett problem är att ekonomiska, militära och andra intressen gör att AI-teknik går framåt mycket fortare.

”Men kan vi inte bara dra ut sladden om en super-AI visar sig vilja skada oss människor?” Kanske. Om det verkligen är en super-AI förstår den människor tillräckligt väl för att veta var gränsen går, och är kapabel att säkra sin överlevnad även om det börjar ske på bekostnad av människor. Vi kan jämföra med fossilindustrin och klimatförändringar: Det står utom allt rimligt tvivel att våra utsläpp av växthusgaser, mycket på grund av fossilindustrin, skadar mänskligheten som helhet – på ett allvarligt och kostsamt sätt. Ändå är vi oförmögna att agera för att stoppa det.

## Andra risker
Några andra risker med allt mer AI beskrivs kortfattat nedan.

* **Skeva maktstrukturer kan befästas.** Eftersom AI-teknik i stor utsträckning utgår från träning på befintlig data finns en tydlig risk att skeva maktstrukturer lever vidare i AI-beslut. Om en AI ska rekommendera lämpliga kandidater till en chefstjänst finns det risk att vita medelålders män får omotiverade fördelar.
* **Mänskligt innehåll kan dränks av AI-innehåll.** Om GPT-3.5 tränats på text motsvarande 57 miljarder människoliv av läsning kan man konstatera att den mesta texten på internet _inte_ skapats av människor. Med allt skickligare AI kommer det bli svårare att hitta text som faktiskt skrivits av en människa, och svårare att veta när man hittat det.
* **Resurser och makt kan fördelas än mer ojämlikt.** Även om AI är förhållandevis billigt att använda är det dyrt att framställa och kräver enorma mängder data. De få aktörer som kan ta fram kraftfulla AI kan få stort inflytande – både i termer av ekonomisk produktion och vad gäller påverkan på information som människor tar del av. De stora generativa AI-modeller som finns tillgängliga idag kommer från bara sex aktörer.[^2]
* **Snabba förändringar på arbetsmarknaden.** AI-utveckling kan leda till att arbetsmarknaden krymper eller att de kompetenser som efterfrågas ändras på ett sätt som gör många arbetslösa. Det kan både skapa problem för de drabbade människorna och oroligheter på samhällsnivå.

[^1]:	https://futureoflife.org/podcast/sean-ekins-on-regulating-ai-drug-discovery/

[^2]:	https://arxiv.org/abs/2301.04655

[1]:	https://www.newscientist.com/article/2278852-drones-may-have-attacked-humans-fully-autonomously-for-the-first-time/