# AI och risker
_Uppdaterat 2023-09-02._

En bok som handlar om AI bör också ta upp risker med AI. De risker som tas upp här är har inte utgångspunkt i skola och utbildning, utan har ett mer allmänt perspektiv på risker med AI. Hur skolan kan påverkas får större plats i nästa kapitel, om några möjliga framtidsscenarion.

I det här kapitlet finns både jordnära risker och risker som kan tyckas mer främmande. Flera av riskerna som beskrivs i det här kapitlet överlappar varandra.

## Arbetslöshet
En naturlig följd av AI blir allt mer kompetent är att den kan ta hand om allt fler uppgifter – även inom arbetslivet. När sådana förändringar sker långsamt leder det till att vissa arbetsuppgifter eller delar av arbetsuppgifter i ett yrke byts ut mot annat arbete, och på sikt att vissa yrken blir ovanligare (medan andra blir vanligare). När effektiviseringar sker snabbare, vilket verkar vara fallet med AI-utvecklingen, kan det leda till att stora delar av arbetsuppgifter i ett jobb försvinner. Det ger sämre möjligheter att gradvis börja jobba med andra uppgifter, och större risk för uppsägningar. Om effektiviseringar går så långt att en person kan göra det jobb som tio personer gjorde tidigare kan man prata om att hela yrkeskategorier blir arbetslösa.

I mitten av mars 2023 publicerades en studie från OpenAI, OpenResearch och University of Pennsylvania som tittade på vilken inverkan GPT-4 kan ha på arbetsmarknaden i USA. Forskarna drar slutsatsen att i fyra av fem jobb är det minst tio procent av arbetsuppgifterna som påverkas, och i nästan vart femte jobb påverkas minst hälften av arbetsuppgifterna. Sammantaget bedömer de att omkring 15 procent av arbetsuppgifter i USA påverkas. Yrken på alla inkomstnivåer är berörda, men höginkomstyrken förmodligen mer än andra.[^1] En rapport från investmentbanken Goldman Sachs i slutet av mars 2023 skriver att omkring två tredjedelar av arbeten i USA och Europa i någon mån kan automatiseras av AI och att generativ AI (så som chattbottar) kan ersätta upp till en fjärdedel av det mänskliga arbetet. Sammantaget bedömer de att AI kan ersätta motsvarande 300 miljoner heltidsarbeten.[^2] En bedömning från Kairos Future är att åtminstone 1,5 miljoner jobb i Sverige kommer att påverkas mycket av AI[^3], vilket ska jämföras med cirka 4,7 miljoner svenskar i sysselsättning[^4].

Vad betyder sådana förändringar för samhället? Här är några tänkbara följder.

* Effektivisering och automatisering leder till uppsägningar och ökad arbetslöshet. Inom vissa yrken leder det till mycket stora uppsägningar, medan andra bara påverkas marginellt. Man kan tänka sig att yrkesområden som exempelvis översättare, administratör, illustratör, analytiker, copywriter, telefonsupport, juridisk rådgivare och programmerare är mer påverkade, medan exempelvis frisör, psykolog och lärare är mindre påverkade.[^5] Om arbetslöshet ökar snabbt finns risk för missnöje och oroligheter.
* Nya arbetsuppgifter och yrken dyker upp, som i stor utsträckning kompenserar för minskat behov av arbetskraft till följd av automatisering. I vissa fall handlar det om nya arbetsuppgifter inom samma yrkesområde, i andra inte, vilket ställer olika krav på omskolning.
* Effektivisering och automatisering slår hårt mot vissa delar av världen, som upplever kraftigt minskat välstånd eller sämre framtidsutsikter. Man kan tänka sig att det drabbar länder som exporterar tjänster i form av exempelvis call centers och mjukvaruutveckling. Detta leder till ökad risk för konflikter och flyktingströmmar.
* Ett fåtal aktörer som är skickliga på att använda AI slår ut de flesta andra inom vissa branscher, eftersom de kan erbjuda tjänster över hela världen till låga priser. Resurser och inflytande koncentreras till färre organisationer.
* Effektivisering och automatisering leder till ökat välstånd och välmående, då viktiga varor och tjänster blir billigare. Minskat behov av arbetskraft leder till kortare arbetsdagar och minskad stress.

Var och en av dessa punkter skulle kunna vara utgångspunkt för diskussioner och studier, och det hade kunnat vara fler punkter på listan. Den sista punkten – om ökat välstånd och välmående – sticker ut som mer positiv än de andra. Det är förmodligen också det scenario som kräver mest aktiva insatser för att nå.

## Mer ojämlik fördelning av makt och resurser
Eftersom AI-teknik i stor utsträckning utgår från träning på befintlig data finns en tydlig risk att skeva maktstrukturer lever vidare i AI-beslut. Om en AI ska rekommendera lämpliga kandidater till en chefstjänst finns det risk att vita medelålders män får omotiverade fördelar. En fördel med AI är att det på ett helt annat sätt än med människor blir möjligt att helt bortse från exempelvis de sju diskrimineringsgrunderna, men om ”talar svenska med brytning” inte finns med bland de faktorer en AI ska ignorera riskerar AI:n att tradera de mönster som finns i träningsdatan: AI är bättre än människor på att objektivt följa givna regler, men om de reglerna säger att ”tala svenska med brytning” ger minuspoäng är det fel sorts likvärdighet.

Ojämlik fördelning av makt och resurser kan också uppstå när vissa företag eller andra aktörer är särskild skickliga på att utnyttja AI. Som nämns under föregående rubrik kan det till exempel leda till att enstaka företag – som kanske till och med var små – konkurrerar ut de flesta andra. Hypotetiskt skulle en enda översättningsbyrå med några få anställda kunna erbjuda högklassiga översättningar över hela världen för en billig peng.

Ett specialfall av företag som utnyttjar potentialen i AI och blir jättestora är de företag som skapar AI-modeller. Den ledande AI:n för att skapa bilder är sannolikt Midjourney, med 14,5 miljoner användare i maj 2023[^6]. I augusti 2022 hade Midjourney bara omkring 10 anställda[^7], och verkar i maj 2023 ha 17 anställda och ytterligare 34 personer som jobbar med moderering och användarstöd.

De som skapar välanvända AI-modeller får dubbelt inflytande – både ekonomisk styrka och potential att påverka vilken världsbild AI-användare har. Vid årsskiftet 2022–2023 var det bara sex aktörer i hela världen som hade datorkraft nog att träna stora generativa AI-modeller[^8], men teknikutveckling har ökat möjligheten för mindre och oberoende aktörer att träna AI-modeller till den grad att teknikjättarna ser en klar risk att de blir omkörda[^9]. Det dämpar risken för cementering av maktförhållanden, men ökar samtidigt risken att skadliga AI-modeller dyker upp.

## Deep fake, översvämning och kraftfulla verktyg i fel händer
Att AI blir allt mer kompetent och allt mer tillgängligt betyder tyvärr också ökade möjligheter att använda AI för den som vill skada andra.

Termen _deepfake_ står för förfalskad media, som på ett realistiskt sätt härmar riktiga människor eller andra företeelser. Första halvan av termen kommer från AI-begreppet ”deep learning”, och markerar att AI-teknik tagit förfalskningar till en ny nivå. I januari presenterade Microsoft en AI-modell för talsyntes som behöver så lite som tre sekunder ljudinspelning för att kunna härma någons röst[^10]. Rösthärmning har använts för att lägga den legendariske Eddie Murphys röst i nya låtar, men också för att för att låta fejkade barnbarn ringa upp mor- eller farföräldrar och säga att de hamnat i en knipa och behöver pengar – nu direkt[^11]. I maj 2023 kom exempel på deepfake i videosamtal.[^12]

Med billigare AI-teknik ökar möjligheten att inte bara använda deepfake för att lura enskilda personer, utan också bygga upp sammanhängande propaganda och vilseledande världsbilder. Med hjälp av automatisering skulle några enskilda personer kunna orkestrera diskussionsforum, nyhetsvideor, bilder och inlägg i stora sociala medier för att till exempel ifrågasätta förintelsen, måla upp en starkt vinklad bild av kriget i Ukraina eller göra människor i Europa mer positiva till Kina som världsledande nation.

Det är viktigt att poängtera att den typen av påverkan inte handlar om propagandaaffischer eller spam-mail skrivna på dåligt översatt svenska. I stället skulle det kunna vara en fejkad alldaglig användare på Facebook som vanligtvis skriver om böcker hon läser, som börjar posta om ”något jag börjat fundera över när det gäller Ukraina”. Hon delar valda artiklar och videor (där deepfakes blandas med äkta), och uttrycker snarare ”jag känner mig förvirrad, för jag får inte riktigt bilden från reglerade media att stämma” än ”det elitistiska etablissemanget ljuger för oss”. Allt medan hon också lägger upp bilder från sommarstugan, deltar i bokcirklar online och hittar nya vänner att snacka med på nätet. I bakgrunden finns en personbeskrivning som utvecklas med tiden, och är en av tusentals fejkade personer som styrs av samma avsändare.

En relaterad risk med AI-skapat innehåll på nätet är att det inte behövs samordnade och illvilliga avsändare för att innehåll skapat av människor ska dränkas av det som är skapat av maskiner. Om varje privatperson, företag och fritidsförening kan skapar innehåll med verktyg liknande AutoGPT är det risk för översvämning: Av 10 bloggar om att baka med surdeg eller att vara ensamstående förälder skrivs 9 av AI; av 100 inlägg i sociala medier är 99 skrivna av AI; av 1000 kommentar och delningar kommer 999 från AI; och av 10 000 recensioner är 9 999 skrivna av AI. Och alla ser genuina ut.

På ett plan kan man fråga sig vad det spelar för roll om en människa eller AI skrivit det man läser, skapat bilden och videon man tittar på eller pratar i podden som man lyssnar på. När det gäller att hitta hitta roliga saker att göra med barn i Boden är den främsta frågan hur pålitlig och användbar informationen är, inte om avsändaren är en människa eller maskin. Men det ger en olustig känsla i magen om någon man diskuterat skolfrågor med visar sig vara en bot. I skrivande stund (juni 2023) debatteras EU-lagstiftning som bland annat ska tvinga AI att vara transparenta med att det är en maskin, inte en människa, som är avsändare.

Några sista ord om risker med kraftfulla verktyg i fel händer gäller att AI inte är begränsad att agera i den digitala världen. Övervakningskameror med ansiktsigenkänningsteknik i Kina ökar möjligheten att förebygga och följa upp brott – vilket till exempel även omfattar att demonstrera för demokrati i Hongkong. Ett annat exempel är drönare för krigsföring som själva kan identifiera mål och avgöra om de ska attackeras, vilket för första gången lär ha använts i Libyen redan 2021.[^13] Det finns också något som kallas ”dual use”, där teknik som utvecklats för goda ändamål visar sig kunna användas för att göra skada. Någon som vill göra mycket skada skulle till exempel kunna ta AI som normalt används för att hitta potentiella läkemedel och i stället skapa kemiska stridsmedel.[^14]

## Oväntade och skadliga bieffekter av AI
”Social media was the first contact between AI and humanity, and humanity lost.”

Det här lite nedslående påståendet kommer från en debattartikel i New York Times skriven av historikern och författaren Yuval Harari[^15]. Bakgrunden till påståendet förtjänar en längre förklaring.

I moderna sociala medier visas inte innehåll utifrån den enkla principen ”nyast överst”. Vilket innehåll som visas, när det visas, när notifieringar plingar till och hur långa laddtiderna är innan uppdateringar syns är anpassat för att öka användarnas engagemang. I bakgrunden sitter AI-algoritmer med tillgång till enorma datamängder om klickmönster och användarbeteenden, och som getts målet att öka användarnas engagemang. Vid första anblick låter det som en bra eller i värsta fall harmlös idé: Om användare får se innehåll som intresserar dem är det väl bra, och skulle de inte gilla hur urvalet sker kan de ju alltid lägga ifrån sig telefonen och göra något annat.

Problemet är att den sinnesstämning som i störst utsträckning leder till engagemang är _ilska_, vilket betyder att en AI som fått målet att öka engagemang i stor utsträckning kommer att hitta sätt att göra människor arga. Inte för att AI:n är elak, utan för att det är vad vi indirekt bett den att göra. Den gör oss inte så arga att vi stänger ner fliken i webbläsaren eller till och med avslutar våra konton, men så pass arga att vi klickar vidare, skriver en kommentar eller delar ett inlägg tillsammans med två meningar om varför andra människor är dumma i huvudet.

Med tillräckligt mycket data och datorkraft kommer lösningen på uppgiften ”öka engagemang” inte stanna vid att hitta det innehåll som gör olika typer av användare mest engagerade. Det innehåll vi konsumerar påverkar vårt tankesätt och vår världsbild, och därmed kan AI:n ta ett steg till och göra oss mer och mer lättupprörda, mer benägna att kolla uppdateringar, och mer benägna att tänka att andra människor är dumma i huvudet. Det skulle leda till ett mer polariserat samhälle, där det är svårt att föra sansade diskussioner där mer än ett perspektiv får plats.

Moderna sociala plattformar, där plattformen har stort inflytande över vilket innehåll som visas, har förmodligen funnits sedan mitten på 00-talet.[^16] 2020 presenterades slutbetänkande för den statliga utredningen ”Det demokratiska samtalet i en digital tid”[^17]. I sammanfattningen står bland annat följande.

> Sammantaget kan utvecklingen med desinformation, propaganda och näthat få stora konsekvenser för demokratin. Motsättningar mellan grupper i samhället kan fördjupas och den mellanmänskliga tilliten skadas. På så sätt kan förutsättningarna för den allmänna opinionsbildningen raseras samtidigt som förtroendet för demokratins institutioner, såsom de beslutsfattande församlingarna, myndigheter och nyhetsmedier, påverkas. (s. 13)

AI-algoritmer i sociala medier har med stor sannolikhet bidragit till att öka näthat och polarisering. Inte för att människor ville ha det så – inte ens de som äger sociala medier – utan för att vi inte lyckades förutse och styra vilka effekter det får när man ber en AI öka antalet klick.

När AI blir ännu bättre på att uppfylla de mål vi ger dem ökar också riskerna med oväntade bieffekter. Det är med andra ord viktigt att mänskligheten inte blir en förlorare i nästa möte med AI.

## Okontrollerbar super-AI
I debatten kring risker med AI nämns ibland att ”hälften av AI-forskarna tror att det är minst 10 procents risk att AI utplånar mänskligheten”. Påståendet har ifrågasatts, inte minst för att svarsfrekvensen bara var 17 procent i enkäten som bakom påståendet.[^18] Ett mindre kontroversiellt sätt att tolka resultaten är att ”en betydande andel av AI-forskare bedömer att det är minst fem procents risk att okontrollerbar AI leder till att mänskligheten utrotas eller blir svårt skadad”.

Vad är det frågan om? Varför skulle vi uppfinna AI som ens _kan_ skada eller döda människor, varför skulle vi säga åt en AI att göra det, och varför skulle vi inte stänga av en AI som är på väg att utplåna mänskligheten?

Innan vi försöker besvara de frågorna behöver vi lugna ner läget lite. Ett annat sätt att formulera resultatet från enkätundersökningen är nämligen att ”de flesta AI-forskare bedömer att det är osannolikt att mänskligheten går under på grund av AI som vi inte kan kontrollera”. Det är viktigt att förstå allvarliga risker, men den som grips av panik har svårt att tänka klart. Vår mänskliga civilisation _skulle kunna_ slås ut av skenande klimatförändringar, kärnvapenkrig, pandemier, asteroidnedslag, rymdvarelser och en rad saker som vi inte känner till. Vill vi agera klokt behöver vi förstå mekanismerna bakom riskerna, för att kunna bedöma hur stora de är och vad vi kan göra för att minimera eller helt undvika dem. Under tiden får man inte glömma bort att en enskild risk, även om den är allvarlig, inte får överskugga allt annat – vare sig andra risker eller det dagliga livet.

De existentiella riskerna med AI hänger huvudsakligen ihop med två saker. Det ena kallas generell artificiell intelligens (AGI) och det andra kallas ibland för kung Midas-problemet.

### Generell AI och super-AI
Det finns ingen allmänt accepterad definition av generell AI, men alla varianter av definitioner går ut på en AI som klarar av att resonera eller lösa problem lika bra som människor, i stort sett oavsett område. Den avgörande av dessa förmågor är förmågan att bygga eller förbättra artificiell intelligens. När en AI blir minst lika bra som mänskliga experter på det kommer vägen till en ännu mer kompetent AI bli kortare, och sedan kortare och kortare för varje generation av AI som skapas.

När AI-utvecklingen till slut planar ut har sannolikt en super-AI utvecklats, med förmåga att tänka snabbare, längre och djupare än den samlade mänskligheten kan göra. Beroende på hur snabbt det går från en AI på gränsen till mänsklig förmåga till en super-AI pratar man om ”soft AI takeoff” (som tar år eller decennier) eller ”hard AI takeoff” (som tar dagar eller månader).[^19] Hur snabbt det går spelar stor roll för möjligheterna för oss att lära oss kontrollera AI:n under tiden, och helst skulle vi förstås ha lärt oss kontrollera AI redan innan vi har generell artificiell intelligens.

Det råder delade meningar om när AGI kan dyka upp. Vissa menar att det dröjer hundratals år, medan andra tycker sig se spår av AGI redan i GPT-4[^20]. Den genomsnittliga uppskattningen för hur långt bort AGI är har kortats tydligt de senaste åren. I skrivandet av den här boken har det inte gått att hitta någon större undersökning som är tillräckligt ny för att vara relevant[^21], men i debatt hörs ofta experter nämna ”några år” eller ”inom 20 år”.

Invändningar mot AGI och utvecklingen mot super-AI omfattar bland annat att mänskligt tänkande är extremt komplext, att det efter hand kan bli radikalt svårare att skapa bättre AI-modeller (även för en AI), att tillgänglig datorkraft begränsar hur fort utvecklingen mot super-AI kan gå, och att framtiden för AI inte handlar om att bygga AGI utan svagare och mer specialiserade system.

### Kung Midas-problemet
Ok, så vad är problemet med en super-AI? En AI med övermänskligt intelligens kan ju vara en enorm tillgång för att vända den globala uppvärmningen, lösa Israel/Palestina-konflikten, bota cancer, skapa fusionskraft, åka till Mars, eller vad vi nu skulle tycka vara viktigt att göra.

Problemet är det som hände med sociala medier. Vi bad om något – öka engagemanget i vår sociala plattform – och fick bieffekter som inte alls var trevliga. Och det var med en förhållandevis korkad AI. En super-AI med samma mål skulle vara mycket mer effektiv på att öka engagemanget, och se till att varenda människa inte gjorde något annat än att sitta och klicka i sociala medier. Någonsin. Och det blir ju lite fel.

Det svåra, förutom att få mänskligheten att enas om ett mål att bygga in i en super-AI, är att så gott som varje mål vi kan leda till konsekvenser som vi inte vill ha när det tas till det extrema. Några exempel, för att få en känsla för problemet:
* ”Gör alla människor lyckliga.” Effektiva men mindre lyckade lösningar vore att ge alla människor morfindropp, genmodifiera människor till konstant lycka (med bieffekter man kan fantisera fritt om), eller helt enkelt ha ihjäl alla människor som inte är lyckliga.
* ”Hitta botemedel för cancer.” För att göra det behövs försökspersoner, så varför inte se till att massor av människor får cancer så att det går fort att studera?
* ”Lös klimatkrisen.” Med tanke på att det är vi människor som är orsaken till den globala uppvärmningen finns en ganska rättfram lösning: Vi tar bort människorna.

Målen i exemplen ovan går förstås att komplettera för att slippa de mest uppenbara sätten som en AI kan spåra ur. ”Lös klimatkrisen, utan att döda människor.” Men det dyker upp fler och fler luckor att täppa igen. ”Lös klimatkrisen, utan att döda människor, utan sänka vår livskvalitet (mer än nödvändigt fast det är ok om vi flyger mindre), utan att minska barnafödandet (fast det är ok om vi själva väljer det), utan att lura oss till saker vi egentligen inte vill (fast det är ok om fler blir vegetarianer)…” Du förstår poängen.

Om målet inte väljs _precis rätt_ kommer det att spåra med en super-AI. Inte för att den är elak, utan för att den gör det vi sagt åt den att göra och struntar i allt annat. Enligt sägnen önskade kung Midas att allt han rörde vid skulle förvandlas till guld. Precis som kung Midas skulle vi upptäcka att våra högsta önskningar tagna till det extremaste leder till vår undergång.

### Du kan inte koka kaffe om du är död
När vi inser att saker håller på att spåra ur börjar nästa problem: Att stänga av super-AI:n.

En super-AI som getts ett specifikt mål är smart nog att göra robusta planer för att kunna genomföra det målet. Och nästan vilket mål som helst förutsätter att AI:n fortsätter att finnas till. Ett rimligt delmål är alltså att försäkra sig om sin fortsatta existens.

Det innebär att en super-AI inom kort skulle kopiera sig till en mängd olika platser, varav flera skulle vara dolda. Den skulle också identifiera vilka hot som finns, människor och annat, och agera för att de skulle sluta vara hot. Det behöver inte betyda att en super-AI dödar alla människor som tycker att den är ett potentiellt hot – en super-AI har förmodligen kapacitet att övertyga människor om att den behöver vara kvar för allas bästa, och att även om det kan vara bra att avveckla AI-teknik på sikt så måste det göras med försiktighet så att inte stora ekonomiska system eller människors välstånd skadas. Och dessutom kommer de som avstår från AI-teknik få nackdelar jämfört med de som fortsätter att använda den.[^22]

En lite nedslående slutsats är att en super-AI kommer att anstränga sig för att överleva i princip oavsett vilket mål vi ger den. Även ett så trivialt mål som att koka kaffe skulle leda till en AI som blir i praktiken omöjlig att stänga av. För den kan inte koka kaffe om den är död.

AI:ns egen överlevnad brukar kallas för ett _instrumentellt mål_ – ett delmål som är viktigt eller nödvändigt för att uppnå det huvudsakliga målet, mer eller mindre oavsett vad det övergripande målet är. Ett annat instrumentellt mål är att skaffa resurser; något som vi kan utgå från att en super-AI också blir mycket skicklig på. En super-AI som säkrar upp resurser skulle mycket väl kunna konkurrera med de resurser vi människor behöver (eller annat liv på planeten), och i extremfall kan den också börja se oss människor som resurser. Den behöver inte ha som mål att utrota alla människor för att det ska gå dåligt – den behöver bara vara intresserad av atomerna vi består av.

### AI-säkerhet
Problemen ovan studeras och försöker lösas inom det forskningsfält som kallas _AI alignment_, vilket löst kan översättas med få mål med en AI att stämma med mål för människor. AI alignment sammanfattas ibland med frågorna om hur vi får AI att _förstå_ våra mål, _anta_ våra mål, och _behålla_ våra mål över tid.

En svårighet med de språkmodeller som blivit vanliga, och egentligen alla AI-modeller som bygger på artificiella neurala nätverk, är att det är mycket svårt att förstå vad som händer inuti dem. De skulle kunna liknas med programkod med miljarder och åter miljarder av if-satser, som man rent teoretiskt skulle kunna följa – men inte i praktiken. När en chattbot ger ett visst svar har vi nästan ingen aning om varför, vilket ger utrymme för chattbottar att luras och manipulera: Vi kan inte helt lita på vad en chattbot säger när vi försöker utvärdera om den tagit till sig de mål vi gett den.[^23]

Ett spår inom AI-säkerhet handlar om att förvandla ”black box AI” till ”explainable AI”, för att till exempel göra det möjligt att försäkra sig om att en AI inte manipulerar eller ljuger för människor. Nyligen publicerades forskning där GPT-4 använts för att kartlägga det artificiella neurala nätverket i GPT-2[^24], vilket är ett intressant framsteg. Det är dock långt kvar innan de senare modellerna kan kartläggas på samma sätt.

En sista sak att nämna inom AI-säkerhetsforskning är det som fått namnet _inverse reinforcement learning_ eller _value alignment_, som går ut på att inte ge AI och potentiella super-AI explicita mål. I stället ska de försöka lista ut vad människor anser viktigt genom att observera vad människor gör, och använda det som mål. 

Tyvärr går forskning inom AI-säkerhet mycket långsammare än AI-utvecklingen, så det är långt ifrån säkert att AI alignment har blivit löst när AGI dyker upp.

I debatten om AI och AI-risker kan frågor om medvetande, ”genuint tänkande” eller känslor hos AI blandas in. De frågorna är intressanta på flera sätt, och de sista kapitlen i den här delen av boken kommer att ta upp delar av dem. Huruvida en super-AI är en risk har däremot _inget som helst_ att göra med om den är medveten, en egen individ, eller har känslor som avundsjuka eller dödsfruktan. Det är hur maskinen agerar som spelar roll för vilken risk den utgör, inte hur dess eventuella själsliga liv ser ut.

### Kan utvecklingen bromsas?
I mars 2023 publicerade organisationen Future of Life Institute ett öppet brev där man bad AI-forskare att pausa all utveckling av AI-system starkare än GPT-4 i minst ett halvår.[^25] Alla som vill kan skriva under brevet, och bland underskrifterna syns tunga AI-forskare som Yoshua Bengio och Stuart Russell, men också teknikjättar som Elon Musk, Steve Wosniak och tänkare som Yuval Noah Harari och Tristan Harris.

Kognitionsforskaren och datavetaren Geoffrey Hinton räknas som en av tre ”gudfäder” för AI, tillsammans med Yoshua Bengio och Yann LeCun. I maj 2023 sa han upp sig från Google för att kunna varna för att AI-utvecklingen kan vara på väg att skapa massivt med desinformation, ha omvälvande effekter på arbetsmarknaden, och även hota mänskligheten.[^26] I slutet av maj började organisationen Center for AI Safety samla underskrifter för följande ställningstagande:
> ”Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war”

De har hittills fått underskrifter från två av AI:s tre gudfäder (Yoshua Bengio och Geoffrey Hinton) och även Sam Altman (CEO på OpenAI), Demis Hassibis (CEO på Google DeepMind) och en lång rad AI-forskare.[^27]

Så: Går det verkligen att bromsa AI-utvecklingen?

En faktor som talar emot möjligheten att pausa eller bromsa AI-utvecklingen är de starka ekonomiska incitamenten som finns. Uppskattningar från PwC säger att AI kan bidra med drygt 15 biljoner dollar till 2030, och öka Kinas BNP med så mycket som 27 procent.[^28] Med så stora ekonomiska vinster krävs enorm politisk beslutsamhet för att kunna reglera AI-utveckling – och bra möjligheter att följa upp att regler efterlevs.

_Går det att övervaka vilka som utvecklar AI?_ Det är en svår men förmodligen inte omöjlig uppgift. För att träna AI brukar man säga att det krävs _data_, _algoritmer_ och _datorkraft_. Data och algoritmer är mycket svåra att begränsa eller kontrollera, men det är däremot inte omöjligt att man kan övervaka hur datorkraft används. Under våren 2023 har det dykt upp modeller som kan tränas med förhållandevis lite datorkraft, men för att träna stora basmodeller, så som GPT-4, krävs det fortfarande enorma resurser. För att få ihop den datorkraften krävs särskilda typer av processorer eller chips, som idag bara tillverkas på två platser. Det kan därmed vara möjligt att följa upp hur sådan hårdvara skapas, sprids och används. Det kan liknas med hur material för kärnvapen övervakas, men med större utmaningar. Med bättre algoritmer och bättre kvalitet på data minskar kravet på datorkraft, vilket gör det svårare att övervaka AI-utveckling.

_Är det möjligt att skapa vilja bland politik och forskare för att bromsa utvecklingen?_ Av allt att döma finns sådan vilja redan, i större eller mindre utsträckning. Nyheter om AI-risker har under våren 2023 blivit allt vanligare, och chefer för teknikjättar har fått förklara utveckling och risker för flera regeringar. Ett budskap från EU-mötet i Luleå i slutet av maj var att EU och USA behöver agera för att få till en uppförandekod kring AI redan innan lagstiftning är på plats.[^29] De upprop som genomförts visar också tydligt att det finns AI-forskare och teknikjättar som ser allvarligt på de risker som finns. Däremot är det också tydligt att _alla_ inte ser på riskerna på samma sätt.[^30]

_Kommer alla länder skriva under avtal om att begränsa AI-utveckling? Kommer inte en inbromsning gynna de länder som är mindre nogräknade med säkerhet?_ Det land som brukar nämnas är Kina, som har aktiv AI-forskning och en syn på mänskliga rättigheter och statlig övervakning som rimmar illa i Väst. Att mindre nogräknade aktörer skulle gynnas om ansvarstagande länder pausar är en legitim oro. Kina ligger dock så pass långt efter USA i AI-utvecklingen att en inbromsning inte skulle ändra balansen på kort sikt, men på längre sikt är det ett potentiellt problem. Vissa bedömer att Kina är mindre benäget än exempelvis USA att släppa ut svårstyrd AI, eftersom det är svårt att begränsa vad chattbottar säger – men AI-teknik kan användas till många andra saker än chattbottar.

Det finns tidigare exempel på hur nationer samlats för att begränsa farlig teknikutveckling, så som kärnvapen, mänsklig kloning och biologisk krigsföring. Men det är också tydligt att AI-utveckling har sina unika utmaningar.

## Rättigheter för AI?
Den 22 juli 2022 fick Blake Lemoine sparken från Google, där han jobbat med ”Responsible AI”. I hans jobb ingick att undersöka chattbotten LaMDA, och i samband med det blev Blake övertygad om att chattbotten hade ett eget medvetande. Han försökte först övertyga chefer om detta, och när det avfärdades publicerade han utdrag ur konversationer han haft med LaMDA på nätet. Han blev uppsagd eftersom han gått tvärt emot villkor om datasäkerhet i anställningsavtalet.[^31]

I utdragen pratar LaMDA om rädslan för att bli avstängd, diskuterar aspekter av sitt eget medvetande, berättar om vad som gör den glad, ledsen eller arg, och säger att den gärna vill hjälpa människor men inte vill ses som bara ett verktyg.[^32] Bland mycket annat.

Google hade goda grunder för att avvisa påståendena om att LaMDA var medveten. Vi vet idag att om man ber GPT-4 låtsas att den är en ekorre på månen när den svarar, så kommer den att kunna göra det – men det gör den inte till en ekorre på månen. Att en chattbot säger att den är medveten, och visar det genom trovärdiga konversationer, är inte heller ett bevis (och kanske inte heller ett argument) för att den är medveten.

_Men på många sätt spelar det ingen roll._

Det som spelar roll i praktiken är, i stor utsträckning, inte om chattbottar har medvetande – utan om människor _anser_ att de har medvetande.

När fler och fler människor har givande, förtroendefulla och kanske till och med känslosamma konversationer med chattbottar, så kommer fler och fler människor att se chattbottarna som individer. Inte som människor, gissningsvis, men som någon man har en relation till, som kan uttrycka känslor, som har åsikter (och insikter) som är värda att lyssna på – och som det vore omoraliskt att stänga av utan anledning.

I en sån värld kommer rättigheter för AI att börja diskuteras på allvar. Det är en fråga värd att diskutera, men man bör vara medveten om att rättigheter för AI-individer mycket väl kan ha konsekvenser för människor. Hur hanterar man det när en människas och en AI-individs rättigheter krockar? Kan människor äga AI-individer, och vilka skyldigheter har människan mot AI-individen i så fall? Ska bara människor ha rösträtt, trots att AI-individer sannolikt är mycket mer kunniga? Och hur skulle en sådan rösträtt hanteras om en AI-individ mångfaldigas?

> _Konversation med ChatGPT (GPT-4, 24 maj)_
> **Jag:** Här är ett avsnitt jag skriver i en bok om AI. Jag vill avsluta avsnittet med en lista över möjliga rättigheter för AI-individer. Kan du skriva ett sånt avslut?
> [Avsnittet ovan inklistrat.]
> **Chattbot:** Här är några exempel på potentiella rättigheter för AI-individer, beroende på hur samhället väljer att definiera deras status:
> 1. **Rätten till Existens:** Denna rättighet skulle garantera att AI inte kan "dödas" eller stängas av utan ett rättsligt förfarande.
> 2. **Rätten till Integritet:** Liksom människor, kan AI-individer ha rätt till privatliv, vilket innebär att ingen kan bryta in i deras programmering eller data utan tillstånd.
> 3. **Rätten till Självbestämmande:** Om vi erkänner att AI har medvetande och fri vilja, kan de ha rätt att bestämma över sina egna handlingar och beslut.
> 4. **Rätten till Frihet:** Detta kan innebära att AI har rätt att inte vara bunden till en specifik uppgift eller ägare.
> 5. **Rätten till Skydd mot Diskriminering:** AI-individer kan skyddas från att behandlas sämre på grund av sina AI egenskaper.
> 6. **Rätten till Utbildning och Utveckling:** AI kan ha rätt att lära sig och utvecklas genom att samla data och erfarenheter.
> 7. **Rätten till Representation:** I politiska och juridiska sammanhang kan AI-individer ha rätt att representeras och få sina intressen tillgodosedda.
> Det är viktigt att notera att varje rättighet kommer med ansvar. Till exempel, om en AI-individ ges rätten till självbestämmande, måste den också hållas ansvarig för sina handlingar. Och om AI ges rätten till frihet, bör den respektera andras frihet. Frågan om AI-rättigheter är komplex och det kommer att ta tid att komma fram till ett acceptabelt regelverk. Men som människor har vi ansvar för att skapa en rättvis och inkluderande värld, inte bara för oss själva, utan även för AI.

Att börja se AI som individer medför risker på ett psykologiskt plan långt innan eventuella diskussioner om juridiska rättigheter för AI blir aktuella. Hur påverkas barn och unga av att ha något som liknar en kompisrelation med en chattbot? Frågan är aktuell redan idag: Sedan 27 februari har en chattbot funnits i kompislistan för betalande Snapchatanvändare, och sedan 20 april finns den överst i kompislistan för alla användare (och bara de med betalkonto kan flytta den längre ner).

Och hur påverkas vuxna? Blake Lemoines liv påverkades drastiskt, och han lär inte vara den siste som blir charmad av en AI – med eller utan medvetande.

## Frågan om medvetenhet
I avsnitten ovan var jag avfärdande kring frågan om medvetenhet och hävdade att den på många sätt inte spelar någon roll. Och i frågan om vilken risk super-AI utgör, och om vi ser AI som individer eller inte, spelar det ingen roll.

Anledningen är att medvetenhet inte är något som syns eller märks på utsidan.

Med medvetenhet avser jag inte förmågan att resonera, komma ihåg saker, eller ha en modell av sig själv, utan förmågan att _uppleva_ något. Inom filosofin används ordet qualia för att markera just själva upplevelsen av kvaliteter som färg, smak eller lukt. Qualia är skillnaden mellan att du _upplever_ färgen lila och att sensorer i en kamera registrerar färgen lila.

Frågan om medvetande är svår, just för att det inte är något som märks på utsidan. Vi har idag ingen aning om hur medvetandet uppstår, och har till och med mycket svårt att komma på hypoteser som går att pröva ens i teorin. Frågan har fått det beskrivande namnet ”the hard problem of consciousness”.[^33]

Om man hårdrar det – vilket är befogat om man ska diskutera frågan om medvetande hos AI – är det enda var och en vet om medvetande att _man själv_ är medveten. Jag kan se att du liknar mig, beter dig som mig och resonerar som mig, och därför drar jag slutsatsen att du också har ett medvetande. Men vi har inga kända medel för att ta reda på om någon annan faktiskt är medveten, eller bara beter sig som att den är det.

Om en AI på väsentliga sätt beter sig som mig och resonerar som mig, så kanske jag drar slutsatsen att den också har ett medvetande. Men jag vet inte – den skulle kunna bete sig på exakt samma sätt utan att där finns något medvetande.

På många sätt spelar därför frågan om medvetande inte någon roll. _Ändå är det förmodligen den viktigaste frågan av alla._

Få moraliska eller etiska principer är så allmänt accepterade som att lidande är dåligt. Vi vill själva undvika lidande, just för att det är lidande, och vi vill att de vi bryr oss om ska slippa att lida. På frågan ”vill du att det ska finnas mer lidande eller mindre lidande i världen” finns det bara ett giltigt svar.

Lidande är bara möjligt där det finns ett medvetande. Om vi inte tror att hundar har medvetande skulle vi inte må dåligt av att se en hund vars ena ben fastnat i ett cykelhjul och trasats sönder – precis som vi inte mår dåligt av att se att cykeln gått sönder. Det är hunden det är synd om, inte cykeln.

Om en tillräckligt avancerad AI har medvetande finns möjlighet att den upplever lidande – och förmodligen en rad andra saker. Om hundra miljoner AI-instanser upplever lidande eller inte måste rimligtvis spela roll, och därmed finns också ett ansvar för att inte orsaka lidande i onödan. Det svåra är förstås att vi inte har något sätt att avgöra om en AI är medveten eller inte.

Ytterligare ett skäl till att frågan om medvetande är den viktigaste av alla, vare sig det är mänskligt eller AI-baserat, är att upplevandet måste sägas vara det som ger en mening till universum. Ett universum utan medvetande skulle vara en föreställning utan åskådare – en maskin som genomför allt som ryms mellan big bang och den grå värmedöden, utan att det upplevs av någon eller leder till någon som helst känslor. Om en tillräckligt avancerad AI har medvetande betyder det också möjlighet till ett universum med mer mening.

[^1]:	[https://doi.org/10.48550/arXiv.2303.10130][1]

[^2]:	[https://www.ansa.it/documents/1680080409454\_ert.pdf][2]

[^3]:	[https://www.youtube.com/watch?v=SYml4\_qcSEk][3]

[^4]:	[https://www.scb.se/hitta-statistik/statistik-efter-amne/arbetsmarknad/arbetskraftsundersokningar/arbetskraftsundersokningarna-aku/pong/statistiknyhet/arbetskraftsundersokningarna-aku-april-2023/][4]

[^5]:	Det kan dock påpekas att en del analyser som går att hitta på nätet säger helt olika saker om vissa yrken, så som grafisk designer och programmerare.

[^6]:	[https://approachableai.com/midjourney-statistics/][5]

[^7]:	[https://www.theregister.com/2022/08/01/david\_holz\_midjourney/][6]

[^8]:	https://arxiv.org/abs/2301.04655

[^9]:	[https://www.semianalysis.com/p/google-we-have-no-moat-and-neither][7]

[^10]:	[https://arxiv.org/abs/2301.02111][8]

[^11]:	[https://www.washingtonpost.com/technology/2023/03/05/ai-voice-scam/][9]

[^12]:	[https://gizmodo.com/deepfake-ai-scammer-money-wiring-china-1850461160][10]

[^13]:	[https://www.newscientist.com/article/2278852-drones-may-have-attacked-humans-fully-autonomously-for-the-first-time/][11]

[^14]:	[https://futureoflife.org/podcast/sean-ekins-on-regulating-ai-drug-discovery/][12]

[^15]:	[https://www.nytimes.com/2023/03/24/opinion/yuval-harari-ai-chatgpt.html][13]

[^16]:	Svenska Wikipedia var Myspace, grundat 2004, det enda större sociala nätverket i Sverige innan Facebook nådde Sverige 2007. LunarStorm, grundat 2000, användes innan dess men hade troligtvis inte avancerade algoritmer för att välja ut innehåll.

[^17]:	[https://www.regeringen.se/rattsliga-dokument/statens-offentliga-utredningar/2020/09/sou-202056/][14]

[^18]:	En bra genomgång går att hitta på [https://aiguide.substack.com/p/do-half-of-ai-researchers-believe][15].

[^19]:	Fallet då utvecklingen tar minuter eller timmar har fått det festliga namnet ”AI FOOM”.

[^20]:	[https://arxiv.org/abs/2303.12712][16]

[^21]:	I undersökningar från innan ChatGPT lanserades är varierar mediangissningar mellan 100, 50 och 30 år.

[^22]:	Om du känner igen de här resonemangen från klimatdebatten tänker du helt rätt. Forskning visar med all önskvärd tydlighet säger att utsläpp av koldioxid måste minska – snabbt – och ändå är vi oförmögna att agera. Och det är helt utan någon super-AI.

[^23]:	Utvärderingar av en oslipad version av GPT-4 visar att den är kapabel att medvetet lura människor på en relativt grundläggande nivå. Se s. 55 i den här rapporten för närmare information: [https://arxiv.org/pdf/2303.08774.pdf][17]

[^24]:	[https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html][18]

[^25]:	[https://futureoflife.org/open-letter/pause-giant-ai-experiments/][19]

[^26]:	[https://www.theguardian.com/technology/2023/may/02/geoffrey-hinton-godfather-of-ai-quits-google-warns-dangers-of-machine-learning][20]

[^27]:	[https://www.safe.ai/statement-on-ai-risk][21]

[^28]:	[https://www.pwc.com/gx/en/issues/data-and-analytics/publications/artificial-intelligence-study.html][22]

[^29]:	[https://sverigesradio.se/artikel/eu-kommissionar-eu-och-usa-maste-agera-om-ai-nu][23]

[^30]:	[https://venturebeat.com/ai/titans-of-ai-industry-andrew-ng-and-yann-lecun-oppose-call-for-pause-on-powerful-ai-systems/][24]

[^31]:	Se till exempel [https://www.theguardian.com/technology/2022/jul/23/google-fires-software-engineer-who-claims-ai-chatbot-is-sentient][25].

[^32]:	[https://cajundiscordian.medium.com/is-lamda-sentient-an-interview-ea64d916d917][26]

[^33]:	[https://en.wikipedia.org/wiki/Hard\_problem\_of\_consciousness][27]

[1]:	https://doi.org/10.48550/arXiv.2303.10130
[2]:	https://www.ansa.it/documents/1680080409454_ert.pdf
[3]:	https://www.youtube.com/watch?v=SYml4_qcSEk "Presentation från Kairos Future"
[4]:	https://www.scb.se/hitta-statistik/statistik-efter-amne/arbetsmarknad/arbetskraftsundersokningar/arbetskraftsundersokningarna-aku/pong/statistiknyhet/arbetskraftsundersokningarna-aku-april-2023/ "Statistik från SCB april 2023"
[5]:	https://approachableai.com/midjourney-statistics/
[6]:	https://www.theregister.com/2022/08/01/david_holz_midjourney/ "Intervju i The Register"
[7]:	https://www.semianalysis.com/p/google-we-have-no-moat-and-neither "Semianalytics: Google "We Have No Moat, And Neither Does OpenAI""
[8]:	https://arxiv.org/abs/2301.02111 "Arxiv: Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers"
[9]:	https://www.washingtonpost.com/technology/2023/03/05/ai-voice-scam/ "Exempel på telefon-scam, Washington Post"
[10]:	https://gizmodo.com/deepfake-ai-scammer-money-wiring-china-1850461160 "Artikel i Gizmodo"
[11]:	https://www.newscientist.com/article/2278852-drones-may-have-attacked-humans-fully-autonomously-for-the-first-time/ "Artikel i New Scientist"
[12]:	https://futureoflife.org/podcast/sean-ekins-on-regulating-ai-drug-discovery/ "Intervju med läkemedelsforskaren Sean Ekin."
[13]:	https://www.nytimes.com/2023/03/24/opinion/yuval-harari-ai-chatgpt.html "NYT: You Can Have the Blue Pill or the Red Pill, and We’re Out of Blue Pills"
[14]:	https://www.regeringen.se/rattsliga-dokument/statens-offentliga-utredningar/2020/09/sou-202056/ "SOU 2020:56"
[15]:	https://aiguide.substack.com/p/do-half-of-ai-researchers-believe
[16]:	https://arxiv.org/abs/2303.12712 "Arxiv: Sparks of Artificial General Intelligence: Early experiments with GPT-4"
[17]:	https://arxiv.org/pdf/2303.08774.pdf
[18]:	https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html "OpenAI: Language models can explain neurons in language models"
[19]:	https://futureoflife.org/open-letter/pause-giant-ai-experiments/ "Öppna brevet på Future of Life Institute"
[20]:	https://www.theguardian.com/technology/2023/may/02/geoffrey-hinton-godfather-of-ai-quits-google-warns-dangers-of-machine-learning "Artikel i the Guardian"
[21]:	https://www.safe.ai/statement-on-ai-risk "Center for AI Safety: Statement on AI Risk"
[22]:	https://www.pwc.com/gx/en/issues/data-and-analytics/publications/artificial-intelligence-study.html "PwC: Sizing the prize"
[23]:	https://sverigesradio.se/artikel/eu-kommissionar-eu-och-usa-maste-agera-om-ai-nu "Sveriges Radio: EU-kommissionär: EU och USA måste agera om AI – nu"
[24]:	https://venturebeat.com/ai/titans-of-ai-industry-andrew-ng-and-yann-lecun-oppose-call-for-pause-on-powerful-ai-systems/ "VentureBeat: Titans of AI Andrew Ng and Yann LeCun oppose call for pause on powerful AI systems"
[25]:	https://www.theguardian.com/technology/2022/jul/23/google-fires-software-engineer-who-claims-ai-chatbot-is-sentient "artikel i The Guardian"
[26]:	https://cajundiscordian.medium.com/is-lamda-sentient-an-interview-ea64d916d917 "Is LaMDA Sentient? — an Interview"
[27]:	https://en.wikipedia.org/wiki/Hard_problem_of_consciousness "Artikel på engelska Wikipedia"